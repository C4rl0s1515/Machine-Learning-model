{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78358ffb",
   "metadata": {},
   "source": [
    "## **1. Importación de librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos.\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52943f55",
   "metadata": {},
   "source": [
    "## **2. Carga de datos `dataset_estudiantes.csv`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1667f8",
   "metadata": {},
   "source": [
    "Se importa el fichero `dataset_estudiantes.csv` en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9887fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/1.raw/dataset_estudiantes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6da58",
   "metadata": {},
   "source": [
    "## **3. Definición de objetivos y variables predictoras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aecc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regresion = df[\"nota_final\"]\n",
    "y_clasificacion = df[\"aprobado\"]       \n",
    "\n",
    "X = df.drop(columns=[\"nota_final\", \"aprobado\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c5acf",
   "metadata": {},
   "source": [
    "En este punto se separa el problema en sus elementos principales, definiendo por un lado las variables objetivo y por otro las variables predictoras. `y_regresion` establece la variable objetivo para la tarea de regresión, mientras que `y_clasificacion` define la variable objetivo para la tarea de clasificación. Por su parte, `X` construye el conjunto de variables predictoras eliminando las columnas objetivo, lo que evita que se produzca una fuga de información y asegura que los modelos aprendan únicamente a partir de las características de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deedab3",
   "metadata": {},
   "source": [
    "## **4. Definición de columnas numéricas y categóricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_num = [\"horas_estudio_semanal\", \"nota_anterior\", \"tasa_asistencia\", \"horas_sueno\", \"edad\"]\n",
    "\n",
    "columns_cat = [\"nivel_dificultad\", \"tiene_tutor\", \"horario_estudio_preferido\", \"estilo_aprendizaje\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1d3de",
   "metadata": {},
   "source": [
    "En este paso se especifica qué columnas de `X` son numéricas y cuáles son categóricas, ya que cada tipo requiere un preprocesamiento distinto. Las columnas numéricas se preparan para aplicar imputación por mediana y escalado, mientras que las columnas categóricas se preparan para imputar valores nulos y posteriormente codificarlas mediante One-Hot Encoding. Esta separación permite aplicar a cada grupo de variables la transformación adecuada dentro del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b7a01",
   "metadata": {},
   "source": [
    "## **5. Tratamiento de valores nulos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb49f5",
   "metadata": {},
   "source": [
    "En este punto se define cómo tratar los valores nulos antes de entrenar los modelos. En el conjunto de datos se han detectado nulos en dos variables categóricas `horario_estudio_preferido` y `estilo_aprendizaje`, y en la variable numérica `horas_sueno`.\n",
    "\n",
    "Para evitar fuga de información y mantener un flujo reproducible, la imputación no se realiza modificando el DataFrame directamente, sino dentro de un `Pipeline`, de forma que los parámetros de imputación se calculen únicamente con el conjunto de entrenamiento para que se apliquen después tanto a train como a test.\n",
    "\n",
    "En las variables numéricas, se decide imputar los valores nulos con la mediana de cada columna, ya que esta medida es robusta frente a posibles valores extremos y representa de forma estable el valor central, en el caso de `horas_sueno` la distribución observada en el EDA es relativamente concentrada, por lo que esta elección resulta la más adecuada.\n",
    "\n",
    "En las variables categóricas, los valores nulos se imputarán con la categoría constante `Desconocido`. Con ello se mantiene explícita la ausencia del dato y se evita sustituir sistemáticamente por la categoría más frecuente. Además, esta opción facilita el modelado al tratar los nulos como una categoría adicional durante el One-Hot Encoding.”\n",
    "\n",
    "Estas decisiones se implementarán mediante `SimpleImputer` dentro del preprocesamiento, junto con la codificación de variables categóricas, asegurando la consistencia y la reproducibilidad en todo el flujo de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b40b4",
   "metadata": {},
   "source": [
    "## **6. Creación del pipeline de variables numéricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_num = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c3893",
   "metadata": {},
   "source": [
    "Para las variables numéricas se define un pipeline que aplica dos operaciones en orden. Primero se imputan los valores nulos utilizando la mediana de cada columna, lo que permite tratar adecuadamente los datos ausentes de forma robusta. Después, se realiza el escalado mediante `StandardScaler` para que todas las variables numéricas queden en una escala comparable, algo muy útil para modelos más sensibles a la magnitud de las variables. En modelos basados en árboles este paso no es imprescindible, pero se mantiene para garantizar un preprocesamiento consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128864f3",
   "metadata": {},
   "source": [
    "## **7. Creación del pipeline de variables categóricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cat = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Desconocido\")),(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a1e43",
   "metadata": {},
   "source": [
    "Para las variables categóricas se construye un pipeline específico que, primero imputa los valores nulos con una categoría constante \"Desconocido\" para conservar la información ausente de las variables. A continuación, se aplica `OneHotEncoder` para convertir las categorías en variables binarias, permitiendo que los modelos trabajen con estas columnas. Además, se configura para que ignore categorías no vistas durante el entrenamiento, evitando de esta forma errores al transformar el conjunto de validación o test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46741ef",
   "metadata": {},
   "source": [
    "## **8. Unión de ambos pipelines con ColumnTransformer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(transformers=[(\"num\", transform_num, columns_num), (\"cat\", transform_cat, columns_cat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99086670",
   "metadata": {},
   "source": [
    "En este paso se unifican los dos pipelines definidos previamente mediante ColumnTransformer, con el objetivo de aplicar transformaciones diferentes según el tipo de variable. De esta forma, el pipeline numérico se aplica únicamente a las columnas numéricas para imputar nulos y escalar, mientras que el pipeline categórico se aplica solo a las columnas categóricas para imputar valores nulos y realizar la codificación One-Hot Encoding. El resultado es un preprocesador único y reutilizable que transforma `X` en una matriz totalmente numérica y lista para el entrenamiento de modelos en las fases posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad68a0",
   "metadata": {},
   "source": [
    "## **9. Comprobación del preprocesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f131234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X original: (1000, 9)\n",
      "Shape X transformada: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "X_transformed = preprocess.fit_transform(X)\n",
    "\n",
    "print(\"Shape X original:\", X.shape)\n",
    "print(\"Shape X transformada:\", X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45088021",
   "metadata": {},
   "source": [
    "La matriz original tiene 1000 registros y 9 variables predictoras. Tras aplicar el preprocesamiento, el número de filas se mantiene en 1000, pero el número de columnas aumenta a 19 debido al One-Hot Encoding de las variables categóricas, que genera columnas binarias para cada categoría. Esta transformación se realiza únicamente como una comprobación técnica de dimensiones. En las fases 3 y 4, el preprocesamiento se ajustará exclusivamente con el conjunto de entrenamiento dentro de un Pipeline, evitando fuga de información y asegurando una evaluación fiable en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d5a85",
   "metadata": {},
   "source": [
    "## **10. Preparación del Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a024f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regresion, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_clasificacion, test_size=0.2, random_state=42, stratify=y_clasificacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885f939",
   "metadata": {},
   "source": [
    "En este paso se realiza la partición del conjunto de datos en entrenamiento y prueba, con el objetivo de evaluar los modelos sobre datos no vistos y obtener una estimación lo más realista posible de su rendimiento. Se generan dos divisiones independientes, una para regresión con `nota_final` como variable objetivo y otra para clasificación con `aprobado`. En el caso de clasificación se utiliza partición estratificada, de modo que se mantiene en train y test la misma proporción de aprobados y suspensos observada en el conjunto original, lo cual es especialmente relevante debido al desbalance de clases detectado en el EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7c534",
   "metadata": {},
   "source": [
    "### **Comprobación final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df855228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresión: (800, 9) (200, 9) \n",
      "\n",
      "Clasificación: (800, 9) (200, 9) \n",
      "\n",
      "% aprobado original: 89.8 %\n",
      "% aprobado train: 89.75 %\n",
      "% aprobado test: 90.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Regresión:\", X_train_reg.shape, X_test_reg.shape, \"\\n\")\n",
    "print(\"Clasificación:\", X_train_clf.shape, X_test_clf.shape, \"\\n\")\n",
    "print(\"% aprobado original:\", y_clasificacion.mean() * 100, \"%\")\n",
    "print(\"% aprobado train:\", y_train_clf.mean() * 100,\"%\" )\n",
    "print(\"% aprobado test:\", y_test_clf.mean() * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad46e6",
   "metadata": {},
   "source": [
    "En esta comprobación final se verifica que el preprocesamiento y la partición de datos se han realizado correctamente. Tras aplicar el ColumnTransformer, el conjunto `X` mantiene los 1000 registros, pero pasa de 9 a 19 columnas debido a la codificación one-hot de las variables categóricas, que genera nuevas variables binarias.\n",
    "\n",
    "Posteriormente, se comprueba la división en entrenamiento y prueba, tanto en regresión como en clasificación se obtienen 800 muestras para entrenamiento y 200 para test, manteniendo las mismas 9 variables predictoras antes de transformar. Por último, se valida que la partición estratificada en clasificación conserva la proporción de la clase aprobado de forma consistente, ya que el porcentaje original es 89,8% y se mantiene prácticamente igual en train 89,75% y en test 90%, lo que asegura una evaluación más representativa del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
